// Databricks notebook source
// MAGIC %md
// MAGIC
// MAGIC # Criminology data structuration
// MAGIC
// MAGIC This notebook contains the steps for further structuration and preparation of the criminology dataset from the Bronze ingested data into Silver area.

// COMMAND ----------

import scala.util.matching.Regex
import org.apache.spark.sql.Column
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC Data is accessed via a mount point in DBFS of the container paths. Authentication is handled via Container Account access keys, which are stored in the Databricks own secret store. The latter also keeps the name of the storage account, so the code is ready for easy reproducibility.
// MAGIC
// MAGIC Workdirs are the following:
// MAGIC
// MAGIC - /mnt/bronze (bronze mount point): Data Lake area with the nearly raw data
// MAGIC - /mnt/silver (silver mount point): Destination for the data generated by this notebook
// MAGIC - /mnt/gold (gold mount point): Data Lake area with final data, used to access the district-zip code catalog
// MAGIC

// COMMAND ----------

val datalake = dbutils.secrets.get(scope="databricks", key="datalake-name")
val accessKey = dbutils.secrets.get(scope="databricks", key="datalake-access-key")
val inPath = "/mnt/bronze"
val outPath = "/mnt/silver"
val goldPath = "/mnt/gold"
val mountedVols = dbutils.fs.mounts.map(_.mountPoint)

if (!mountedVols.contains(inPath)) {
  dbutils.fs.mount(
    source = f"wasbs://bronze@${datalake}.blob.core.windows.net/",
    mountPoint = inPath,
    extraConfigs = Map(f"fs.azure.account.key.${datalake}.blob.core.windows.net" -> accessKey)
  )
}

if (!mountedVols.contains(outPath)) {
  dbutils.fs.mount(
    source = f"wasbs://silver@${datalake}.blob.core.windows.net/",
    mountPoint = outPath,
    extraConfigs = Map(f"fs.azure.account.key.${datalake}.blob.core.windows.net" -> accessKey)
  )
}

if (!mountedVols.contains(goldPath)) {
  dbutils.fs.mount(
    source = f"wasbs://gold@${datalake}.blob.core.windows.net/",
    mountPoint = goldPath,
    extraConfigs = Map(f"fs.azure.account.key.${datalake}.blob.core.windows.net" -> accessKey)
  )
}

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC The year and quarter of the corresponding data are parametrized, allowing to only process the partition from Bronze area. The job also receives the auditory data that will be added for the final table. The zip codes dictionary is also loaded.

// COMMAND ----------

val year = dbutils.widgets.get("year").toInt
val quarter = dbutils.widgets.get("quarter").toInt
val dataDate = dbutils.widgets.get("dataDate")
val runId = dbutils.widgets.get("runId")

val zipsDict = spark.read
  .option("header", true)
  .option("sep", ",")
  .csv(f"${goldPath}/zips/zip_dict.csv")

val rawDf = spark.read
  .parquet(f"${inPath}/crime")
  .where(f"year = '${year}' AND q = '${quarter}'")

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC ## Disaggregation of Regions
// MAGIC
// MAGIC The original datasets contain nested geographic information in a "region" column, with the following hierarchy:
// MAGIC
// MAGIC - One row indicates the name of the autonomous community, the values of the different types of crime indicate the total number of occurrences in it
// MAGIC - The next row indicates the name of the province, again the values of the crime categories are the total data in the province
// MAGIC - The subsequent rows contain the municipalities of the province. In this case, the values are the incidents in these, without any aggregation
// MAGIC - The next row after the last district of the province contains the next province of the autonomous community, or the next community if it does not exist, repeating the hierarchy
// MAGIC
// MAGIC This creates problems in accessing the information, as it is not possible to perform aggregation or disaggregation operations on the data at different geographic levels or time frames. Consequently, a restructuring is carried out so that:
// MAGIC
// MAGIC - The autonomous community and the province are new columns
// MAGIC - Each row representing a district contains in these columns the information related to its province and autonomous community
// MAGIC - The rows only represent the hierarchy level of the municipalities

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC First, columns are created to contain the provinces and autonomous communities. These are extracted based on the detection of the format with which each level of the hierarchy is written in the data. The resulting attributes will have the value of the province or community in the row where it appears in the region column, and the rest as null values.

// COMMAND ----------

def addRegionColumns(df: DataFrame): DataFrame = df.withColumn(
    "state", when(col("region") === upper(col("region")), 
    trim(col("region"))))
  .withColumn("province", when(
    !trim(col("region")).startsWith("-") && col("state").isNull, 
    trim(col("region"))))
  .withColumn("district", when(
    col("region").contains("Municipio") || col("region").contains("Isla"),
    trim(col("region"))))

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC With the province and autonomous community markers already created, null values are filled with the last reported value in the column. Given the extraction method applied previously, this means that each district will have its corresponding province and autonomous community assigned. To that end, window functions are applied to the new columns. A serial id is required to keep row order when creating the window, as there is no column that enables partitioning nor ordering without losing the hierarchical information. Then, records where the district is reported are filtered, discarding the rows that originally indicated higher levels of the region hierarchy.

// COMMAND ----------

def deaggregateRegions(df: DataFrame): DataFrame = df.withColumn("serial", monotonically_increasing_id())
  .withColumn("state", last("state", ignoreNulls = true).over(Window
    .orderBy("serial")
    .rowsBetween(Long.MinValue, 0)))
  .withColumn("province", last("province", ignoreNulls = true).over(Window
    .orderBy("serial")
    .rowsBetween(Long.MinValue, 0)))
  .filter(col("district").isNotNull || col("state").contains("CIUDAD AUTÓNOMA"))
  .drop("region", "serial")

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC ## Normalization of location names
// MAGIC
// MAGIC With the municipalities identified, it is necessary to apply normalization to the name formats. This is because various nomenclatures coexist. For example, some cases place the suffix, such as articles, in parentheses at the end of the name. For example, "El Ejido" could be represented as "Ejido (El)". Other aspects include the presence of the region name in a language other than Spanish or differences in uppercase/lowercase.
// MAGIC
// MAGIC The steps taken are:
// MAGIC - Remove explicit prefixes indicating the type of geographic location (such as "Municipio", which is already indicated by the column it is in)
// MAGIC - Reconstruct names to place articles and prefixes located in parentheses in their original place
// MAGIC - Normalize the number of spaces to one and remove characters like hyphens at the beginning
// MAGIC - Apply uppercase and lowercase reformatting so that each word starts with a capital letter

// COMMAND ----------

val nameStandardization = Map(
  "Eivissa" -> "Ibiza",
  "ARABA" -> "ÁLAVA"
)

val standardizeNames = udf((reg: String) => {
  if (reg != null) {
    val translatedAndSpaceClean = reg.split("/").head
      .replaceAll("\\s+", " ")
      .replace("- ", "")
      .replaceAll("Municipio\\sde\\s|Provincia\\sde\\s", "")
    val pattern = "^(.*)\\s\\((.*)\\)".r
    val standardized = pattern.findFirstMatchIn(translatedAndSpaceClean) match {
      case Some(m) =>
        val name = m.group(1).trim
        val prefix = m.group(2).trim
        val prefixPattern = "(Isla\\sde)\\s(.*)".r

        prefixPattern.findFirstMatchIn(name) match {
          case Some(s) => s"${s.group(1).trim} $prefix ${s.group(2).trim}"
          case None => s"$prefix $name"
        }
      case None => translatedAndSpaceClean    
    }
    nameStandardization
      .collectFirst { case (key, value) if standardized.contains(key) => (key, value) }
      .map { case (key, value) => standardized.replace(key, value) }
      .getOrElse(standardized)
      .split("(?<=[\\s\\-'])")
      .map(w => if (w.nonEmpty) w.head.toUpper + w.tail.toLowerCase else w)
      .mkString
  }
  else null
})

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC In addition to standardization, a propagation of region names is applied in case one is empty. For example, autonomous cities are not divided by provinces and districts, and some islands are at the same time provinces, hence they would have the same value among some of the columns.

// COMMAND ----------

def standardizeLocations(df: DataFrame): DataFrame = df.withColumn("state", standardizeNames(col("state")))
  .withColumn("province", coalesce(
    standardizeNames(col("province")),
    col("state")))
  .withColumn("district", coalesce(
    standardizeNames(col("district")), 
    col("province"), 
    col("state")))

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC ## Reformatting of Criminology Categories
// MAGIC
// MAGIC In the input data, crime types are reported as columns, one per category. This leads to problems when querying the information and performing aggregations. An unpivoting step is performed to obtain a new format in which each district has a row for each crime category and a new column indicating the number of occurrences.

// COMMAND ----------

val idCols = Array("year", "q", "state", "province", "district", "zip", "data_date", "run_id", "run_date")

def crimesToCategory(df: DataFrame): DataFrame = df.unpivot(
  idCols.map(col),
  df.columns.diff(idCols).map(c => col(c).cast("integer")).toArray,
  "crime_category",
  "no_occurrences"
)

// COMMAND ----------

// MAGIC %md
// MAGIC  
// MAGIC ## Application of Transformations
// MAGIC
// MAGIC The previous steps are applied in the described order to obtain the data with the desired structure. Two intermediate steps should be noted which, due to their simplicity, are directly integrated into the flow:
// MAGIC
// MAGIC - Cross-reference with the postal code dictionary: This is done to keep only those records that correspond to known locations, so they can be used in subsequent analyses. The step happens once locations have been standardized, to avoid missing records due to subtle differences, and before the unpivot, reducing the number of data that has to be moved and then filtered, if done backwards
// MAGIC - Disaggregation of data on thefts in facilities: Originally, the column corresponding to thefts in facilities also includes the number of thefts inside homes. Both columns are subtracted prior to the unpivoting step to obtain non-redundant measures.

// COMMAND ----------

val finalDf = rawDf.transform(addRegionColumns)
  .transform(deaggregateRegions)
  .transform(standardizeLocations)
  .join(zipsDict, Seq("district"))
  .withColumn("burglary_in_facilities", col("burglary_in_facilities") - col("burglary_in_homes"))
  .transform(crimesToCategory)

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC Auditory data is added prior writing. Data is partitioned by the temporal information (year and quarter) and written using the dynamic partition overwrite to only replace the corresponding data, allowing for reprocesses. The Silver path in this case isn't the final destination of crime data, as quarter registries accumulate the counts of the previous quarters, which requires posterior processing.

// COMMAND ----------

spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")

finalDf.withColumn("run_id", lit(runId))
  .withColumn("data_date", lit(dataDate))
  .withColumn("run_date", current_timestamp())
  .write.mode("overwrite")
  .partitionBy("year", "q")
  .parquet(f"${outPath}/crime_with_aggregates")