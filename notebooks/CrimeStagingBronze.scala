// Databricks notebook source
// MAGIC %md
// MAGIC
// MAGIC # Criminology Data Ingestion
// MAGIC
// MAGIC This notebook contains the steps to ingest the criminology dataset into the first data lake-governed zone: Bronze.

// COMMAND ----------

import org.apache.spark.sql.functions._
import org.apache.spark.sql.DataFrame

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC Data is accessed via a mount point in DBFS of the container paths. Authentication is handled via Container Account access keys, which are stored in the Databricks own secret store. The latter also keeps the name of the storage account, so the code is ready for easy reproducibility.
// MAGIC
// MAGIC Workdirs are the following:
// MAGIC
// MAGIC /mnt/in (staging mount point): Directory inside the staging container in which initial files are placed
// MAGIC /mnt/bronze (bronze mount point): Destination for the data generated by this notebook

// COMMAND ----------

val datalake = dbutils.secrets.get(scope="databricks", key="datalake-name")
val accessKey = dbutils.secrets.get(scope="databricks", key="datalake-access-key")
val inPath = "/mnt/in"
val outPath = "/mnt/bronze"
val mountedVols = dbutils.fs.mounts.map(_.mountPoint)

if (!mountedVols.contains(inPath)) {
  dbutils.fs.mount(
    source = f"wasbs://staging@${datalake}.blob.core.windows.net/tmp",
    mountPoint = inPath,
    extraConfigs = Map(f"fs.azure.account.key.${datalake}.blob.core.windows.net" -> accessKey)
  )
}

if (!mountedVols.contains(outPath)) {
  dbutils.fs.mount(
    source = "wasbs://bronze@lagodelospanchitos.blob.core.windows.net/",
    mountPoint = outPath,
    extraConfigs = Map(f"fs.azure.account.key.${datalake}.blob.core.windows.net" -> accessKey)
  )
}

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC As staging is a non-governed zone, the file name currently being processed is parameterized, as well as auditory data to add to the resulting table. The standard separator for CSV read is the *pipe* '|' character, as some sources contain various common separators in the column names.

// COMMAND ----------

val fileName = dbutils.widgets.get("fileName")
val dataDate = dbutils.widgets.get("dataDate")
val runId = dbutils.widgets.get("runId")

val rawDf = spark.read
  .option("header", true)
  .option("sep", "|")
  .csv(f"${inPath}/${fileName}")

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC In suc input file, the temporal information is located as the name of the first column, which actually contains the regions where the data was collected. The date string includes the months in which the reading occurred and the year, all in plain text format in Spanish. Month ranges end up in quarters, so the extraction logic is:
// MAGIC
// MAGIC - Separate the year and the months
// MAGIC - Define which quarter of the year the input dataset corresponds to based on the end month of the records
// MAGIC - Keep the year and quarter to generate new columns that format the temporal information at the record level

// COMMAND ----------


val timeIndicator = rawDf.columns.head
val timeRanges = timeIndicator.split(" ")
val quarter = timeRanges.head.split("-").last.toLowerCase match {
  case "marzo" => 1
  case "junio" => 2
  case "septiembre" => 3
  case "diciembre" => 4
}
val year = timeRanges.last.toInt

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC Aside from the issue of relevant data contained in a column name, the global naming format is not compliant with parquet's column naming requirements. Here, a column mapping is defined both to remove these incompatibilities and to standardize name styles among data sources.

// COMMAND ----------

val colsMapping = Map(
  timeIndicator -> "region",
  "1. Homicidios dolosos y asesinatos consumados" -> "murders",
  "2. Homicidios dolosos y asesinatos en grado tentativa" -> "murder_attempts",
  "3. Delitos graves y menos graves de lesiones y riña tumultuaria" -> "injuries_riots",
  "4. Secuestro" -> "kidnappings",
  "5.1. Agresión sexual con penetración" -> "rapes",
  "5.2. Resto de delitos contra la libertad e indemnidad sexual" -> "other_sexual_crimes",
  "6. Robos con violencia e intimidación" -> "violent_robbery",
  "7. Robos con fuerza en domicilios, establecimientos y otras instalaciones" -> "burglary_in_facilities",
  "7.1. Robos con fuerza en domicilios" -> "burglary_in_homes",
  "8. Hurtos" -> "thefts",
  "9. Sustracciones de vehículos" -> "vehicle_theft",
  "10. Tráfico de drogas" -> "drug_smuggling",
)

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC Prior to the renaming, string-based standardization is required as in some cases the same name has slight format variations, like the amount of spaces and punctuation usage.

// COMMAND ----------

def standardizeColumns(df: DataFrame): DataFrame = df.columns.foldLeft(df)({
  case (tmp, column) =>
    if (column.contains(".-")) tmp.withColumnRenamed(column, column.replace(".-", ". ").replaceAll("\\s+", " "))
    else tmp
})

val withStandardizedColsDf = rawDf.transform(standardizeColumns)
  .withColumnsRenamed(colsMapping)

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC Some origin sheets and CSV were identified to contain blank rows at the end of the data, which are filtered out by checking if all columns have missing data. The extracted temporal data (year and quarter) is added only after the filtering, as it would otherwise corrupt the all-nulls logic.

// COMMAND ----------

val finalDf = withStandardizedColsDf
  .filter(withStandardizedColsDf.columns.map(col(_).isNotNull).reduce(_ || _))
  .withColumn("year", lit(year))
  .withColumn("q", lit(quarter))

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC Auditory data is added prior writing. Data is partitioned by the temporal information (year and quarter) and written using the dynamic partition overwrite to only replace the corresponding data, allowing for reprocesses.

// COMMAND ----------

spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")

finalDf.withColumn("run_id", lit(runId))
  .withColumn("data_date", lit(dataDate))
  .withColumn("run_date", current_timestamp())
  .write.mode("overwrite")
  .partitionBy("year", "q")
  .parquet(f"${outPath}/crime")

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC Finally, the extracted temporal information is returned to the caller, enabling partition-based read on subsequent processes.

// COMMAND ----------

dbutils.notebook.exit(f"""{
  "year": "${year}",
  "quarter": "${quarter}"
}""")