// Databricks notebook source
// MAGIC %md
// MAGIC
// MAGIC # Demography Data Ingestion
// MAGIC
// MAGIC This notebook contains the steps to ingest the demography dataset into the first datalake-governed zone: Bronze.

// COMMAND ----------

import org.apache.spark.sql.functions.{col, lit, current_timestamp}

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC Data is accessed via a mount point in DBFS of the container paths. Authentication is handled via Container Account access keys, which are stored in the Databricks own secret store. The latter also keeps the name of the storage account, so the code is ready for easy reproducibility.
// MAGIC
// MAGIC Workdirs are the following:
// MAGIC
// MAGIC - /mnt/in (staging mount point): Directory inside the staging container in which initial files are placed
// MAGIC - /mnt/bronze (bronze mount point): Destination for the data generated by this notebook

// COMMAND ----------

val datalake = dbutils.secrets.get(scope="databricks", key="datalake-name")
val accessKey = dbutils.secrets.get(scope="databricks", key="datalake-access-key")
val inPath = "/mnt/in"
val outPath = "/mnt/bronze"
val mountedVols = dbutils.fs.mounts.map(_.mountPoint)

if (!mountedVols.contains(inPath)) {
  dbutils.fs.mount(
    source = f"wasbs://staging@${datalake}.blob.core.windows.net/tmp",
    mountPoint = inPath,
    extraConfigs = Map(f"fs.azure.account.key.${datalake}.blob.core.windows.net" -> accessKey)
  )
}

if (!mountedVols.contains(outPath)) {
  dbutils.fs.mount(
    source = f"wasbs://bronze@${datalake}.blob.core.windows.net/",
    mountPoint = outPath,
    extraConfigs = Map(f"fs.azure.account.key.${datalake}.blob.core.windows.net" -> accessKey)
  )
}

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC As staging is a non-governed zone, the file name currently being processed is parameterized, as well as auditory data to add to the resulting table. The standard separator for CSV read is the *pipe* '|' character, as some sources contain various common separators in the column names.

// COMMAND ----------

val fileName = dbutils.widgets.get("fileName")
val dataDate = dbutils.widgets.get("dataDate")
val runId = dbutils.widgets.get("runId")

val rawDf = spark.read
  .option("header", true)
  .option("sep", "|")
  .csv(f"${inPath}/${fileName}")

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC Columns are mapped to avoid invalid characters for the parquet column naming and to standardize the nomenclature along the data sources.

// COMMAND ----------

val renamedDf = rawDf.withColumnsRenamed(Map(
    "RegiÃ³n" -> "region",
    "Hombres" -> "men",
    "Mujeres" -> "women"))

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC Some origin sheets and CSV were identified to contain blank rows at the end of the data, which are filtered out by checking if all columns have missing data. Auditory data is added only after the filtering, otherwise it would corrupt the all-nulls logic.

// COMMAND ----------

val finalDf = renamedDf.filter(renamedDf.columns.map(col(_).isNotNull).reduce(_ || _))
  .withColumn("run_id", lit(runId))
  .withColumn("data_date", lit(dataDate))
  .withColumn("run_date", current_timestamp())

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC Data is written using the dynamic partition overwrite to only replace the corresponding data, allowing for reprocesses. In this case, partitioning is applied over the process date as temporal information has not yet been properly extracted from data (it requires further structuration that is performed in posterior stages).

// COMMAND ----------

spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")

finalDf.write.mode("overwrite")
  .partitionBy("data_date")
  .parquet(f"${outPath}/demographic")