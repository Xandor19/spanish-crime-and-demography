// Databricks notebook source
// MAGIC %md
// MAGIC
// MAGIC # Criminology data structuration
// MAGIC
// MAGIC This notebook contains the steps for deaggregating the crime counts between quarters of the intermediate Silver criminology table.

// COMMAND ----------

import org.apache.spark.sql.functions.{when, col, lit, current_timestamp}

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC Data is accessed via a mount point in DBFS of the container paths. Authentication is handled via Container Account access keys, which are stored in the Databricks own secret store. The latter also keeps the name of the storage account, so the code is ready for easy reproducibility.
// MAGIC
// MAGIC There is only one workdir involved:
// MAGIC
// MAGIC - /mnt/silver (silver mount point): Both origin (the intermediate table with aggregates) and destination of the data generated by this notebook

// COMMAND ----------

val datalake = dbutils.secrets.get(scope="databricks", key="datalake-name")
val accessKey = dbutils.secrets.get(scope="databricks", key="datalake-access-key")
val silverPath = "/mnt/silver"
val mountedVols = dbutils.fs.mounts.map(_.mountPoint)

if (!mountedVols.contains(silverPath)) {
  dbutils.fs.mount(
    source = f"wasbs://silver@${datalake}.blob.core.windows.net/",
    mountPoint = silverPath,
    extraConfigs = Map(f"fs.azure.account.key.${datalake}.blob.core.windows.net" -> accessKey)
  )
}

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC The year and quarter of the corresponding data are parametrized, allowing to only process the partition from Silver area. The job also receives the auditory data that will be added for the final table.

// COMMAND ----------

val year = dbutils.widgets.get("year").toInt
val quarter = dbutils.widgets.get("quarter").toInt
val dataDate = dbutils.widgets.get("dataDate")
val runId = dbutils.widgets.get("runId")

val rawDf = spark.read
  .parquet(f"${silverPath}/crime_with_aggregates")
  .where(f"year = ${year} AND q = ${quarter}")

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC As quarters contain aggregates from the beggining of the year, the one prior to the quarter loaded in the current process is also obtained. Lazy evaluation is used so Spark plan doesn't accounts for this operation in case of receiving the 1st quarter (e.g, no deaggregation required).
// MAGIC
// MAGIC Deaggregation is done by substracting the occurrences column of the current partition with the prior one. As aggregated data was already kept in the intermediate table, no further computation is required.

// COMMAND ----------

lazy val previousAccumulates = spark.read
  .parquet(f"${silverPath}/crime_with_aggregates")
  .select("zip", "crime_category", "no_occurrences")
  .where(f"year = '${year}' AND q = '${quarter - 1}'")
  .withColumnRenamed("no_occurrences", "previous_occurences")

val deaggregatedDf =
  if (quarter > 1) rawDf.join(previousAccumulates, Seq("zip", "crime_category"), "left")
    .withColumn("no_occurrences", when(
        col("no_occurrences") - col("previous_occurences") >= 0,
        col("no_occurrences") - col("previous_occurences"))
      .otherwise(col("no_occurrences")))
    .drop("previous_occurences")
  else rawDf

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC Auditory data is added to the resulting table.

// COMMAND ----------

val finalDf = deaggregatedDf.withColumn("run_id", lit(runId))
  .withColumn("data_date", lit(dataDate))
  .withColumn("run_date", current_timestamp())

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC Data is partitioned by the temporal information (year and quarter) and written using the dynamic partition overwrite to only replace the corresponding data, allowing for reprocesses.

// COMMAND ----------

spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")

finalDf
  .write.mode("overwrite")
  .partitionBy("year", "q")
  .parquet(f"${silverPath}/crime")